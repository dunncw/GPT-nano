{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJpXpmjEYC_T"
      },
      "source": [
        "## Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('input.txt', <http.client.HTTPMessage at 0x1ddfdcc1b50>)"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# download the tiny shakespeare dataset from this web address 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "# and save it in the same directory as this notebook\n",
        "import urllib.request\n",
        "url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "filename = 'input.txt'\n",
        "urllib.request.urlretrieve(url, filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "outputs": [],
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "ed819dd0-72e5-40a6-d2ed-928ff73bfda6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ],
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "25ca7adc-b8c0-42d1-b08c-e0863c5c314e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "f34e94a9-5b44-4cf3-885b-986731929109"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "86fcc21c-2cf7-40d9-cd7b-b5a253da4459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "db7297cc-36a9-4fae-e941-e7bb9e0e91d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "outputs": [],
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "bf23c586-1d33-4af1-b63d-ce6f90b0a528"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "588663aa-1de5-4ef7-aba0-4a96fe828353"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "4ea8e8a0-443c-49bb-b3bf-ba36e1712999"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "a650f8dc-da81-400b-bc59-0a595487fdb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ],
      "source": [
        "print(xb) # our input to the transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "5de90b1b-4603-428a-f571-fe4bd3c45436"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "outputs": [],
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "42ded55c-2983-4d91-c528-675b2edfa849"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.65630578994751\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100): # increase number of steps for good results... \n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0ad6f9d2-ad58-4498-a5f8-6f31407bb18b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "oTo.JUZ!!zqe!\n",
            "xBP qbs$Gy'AcOmrLwwt\n",
            "p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\n",
            "rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\n",
            "ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\n",
            "SV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\n",
            "tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\n",
            "pSPYgCuCJrIFtb\n",
            "jQXg\n",
            "pA.P LP,SPJi\n",
            "DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\n",
            "D.?\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XinV8nmAnmKN"
      },
      "source": [
        "## The mathematical trick in self-attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "d981f6d4-ac08-4ec2-8284-82f5fa1e0815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "8bf3ff5f-565e-48b8-de8e-7272706c8e12"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "outputs": [],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "eaf6ab61-dff1-4bb7-e623-47f692bad5f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "080b500d-8110-4602-fcef-7d6f2ebfc6bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "07b587dd-a91c-4bb0-d7f1-e247cd5dacb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "6d2c569b-7922-451f-9934-0fc564678d17"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "execution_count": 77,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5CvobiQ0pLr"
      },
      "source": [
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "outputs": [],
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "0c5b9cd0-af8a-4564-fbad-41d844e54822"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "3541ca1a-7447-4ef7-835e-81824aebc1b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "a687a222-5a2c-4cdb-c1bf-17cd05b45b69"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wei.var()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "f07da2f1-10bb-4a7a-bcaa-578587977d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "5d8b910a-6192-44ba-ebb2-497d88e0b629"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "929ceb78-a639-41d6-aac7-12997b5c93f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(100)\n",
        "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7720fa58-0478-4e8a-86a7-502d4cce9443"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "6368ece0-600e-417d-8a91-7c1e5d750ba8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "outputs": [],
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      },
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "961304cd-e379-40d4-dd56-8de0b91d2861"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4194, val loss 2.4335\n",
            "step 400: train loss 2.3499, val loss 2.3562\n",
            "step 500: train loss 2.2969, val loss 2.3135\n",
            "step 600: train loss 2.2415, val loss 2.2507\n",
            "step 700: train loss 2.2058, val loss 2.2195\n",
            "step 800: train loss 2.1640, val loss 2.1873\n",
            "step 900: train loss 2.1234, val loss 2.1495\n",
            "step 1000: train loss 2.1027, val loss 2.1299\n",
            "step 1100: train loss 2.0699, val loss 2.1184\n",
            "step 1200: train loss 2.0394, val loss 2.0800\n",
            "step 1300: train loss 2.0252, val loss 2.0648\n",
            "step 1400: train loss 1.9937, val loss 2.0388\n",
            "step 1500: train loss 1.9701, val loss 2.0301\n",
            "step 1600: train loss 1.9642, val loss 2.0484\n",
            "step 1700: train loss 1.9429, val loss 2.0134\n",
            "step 1800: train loss 1.9095, val loss 1.9946\n",
            "step 1900: train loss 1.9104, val loss 1.9875\n",
            "step 2000: train loss 1.8846, val loss 1.9934\n",
            "step 2100: train loss 1.8750, val loss 1.9755\n",
            "step 2200: train loss 1.8628, val loss 1.9624\n",
            "step 2300: train loss 1.8568, val loss 1.9532\n",
            "step 2400: train loss 1.8433, val loss 1.9442\n",
            "step 2500: train loss 1.8164, val loss 1.9428\n",
            "step 2600: train loss 1.8270, val loss 1.9383\n",
            "step 2700: train loss 1.8131, val loss 1.9336\n",
            "step 2800: train loss 1.8064, val loss 1.9210\n",
            "step 2900: train loss 1.8060, val loss 1.9304\n",
            "step 3000: train loss 1.7948, val loss 1.9184\n",
            "step 3100: train loss 1.7686, val loss 1.9169\n",
            "step 3200: train loss 1.7542, val loss 1.9076\n",
            "step 3300: train loss 1.7598, val loss 1.9074\n",
            "step 3400: train loss 1.7540, val loss 1.8969\n",
            "step 3500: train loss 1.7382, val loss 1.8988\n",
            "step 3600: train loss 1.7266, val loss 1.8896\n",
            "step 3700: train loss 1.7312, val loss 1.8842\n",
            "step 3800: train loss 1.7217, val loss 1.8955\n",
            "step 3900: train loss 1.7222, val loss 1.8723\n",
            "step 4000: train loss 1.7142, val loss 1.8626\n",
            "step 4100: train loss 1.7106, val loss 1.8749\n",
            "step 4200: train loss 1.7050, val loss 1.8679\n",
            "step 4300: train loss 1.7026, val loss 1.8525\n",
            "step 4400: train loss 1.7082, val loss 1.8677\n",
            "step 4500: train loss 1.6889, val loss 1.8518\n",
            "step 4600: train loss 1.6860, val loss 1.8330\n",
            "step 4700: train loss 1.6827, val loss 1.8422\n",
            "step 4800: train loss 1.6674, val loss 1.8451\n",
            "step 4900: train loss 1.6679, val loss 1.8345\n",
            "step 4999: train loss 1.6652, val loss 1.8260\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Shall not the diserablazed enamined.\n",
            "\n",
            "MARCILLAUME:\n",
            "WhithieN will, God?\n",
            "We that ane away, my feansuar, zorn heavens\n",
            "Makfultie ble my would but\n",
            "With ensent, will is the overs, and the now on you sou lesind that.\n",
            "His my now by atcher is plawey. Hush, sIen most blaves\n",
            "Morthly, demperakenom,---on her evily, what\n",
            "For His The shire strong of his but\n",
            "that nobrurt first; if his shate.\n",
            "\n",
            "RICHUERROY:\n",
            "Fiels?\n",
            "\n",
            "KING HENRY VI:\n",
            "Hark, but and day Warwick the huis courtear teyour-bust,\n",
            "And for his no tongue mary.\n",
            "You contranthm so a cempret-spend;\n",
            "But with reason; stay, in with the beggethin untiffery.\n",
            "Which might. \n",
            "IEn Rome, Marward.\n",
            "\n",
            "LUCIO:\n",
            "Abently the did the best:\n",
            "And meety stieve turse: you fear talk:\n",
            "Whith loves, strike, no duke in,\n",
            "To may I want beak that fravint with some.\n",
            "\n",
            "CORIOLANUS:\n",
            "What lack.\n",
            "\n",
            "PRINCE:\n",
            "Fis himself?\n",
            "Their his throngen, do thity sevence?\n",
            "\n",
            "COMIGONUS:\n",
            "My streasure, will, and wilte that the must perplees kings that O\n",
            "For fit.\n",
            "\n",
            "BENVINTH:\n",
            "You the glet us\n",
            "you helpose on the with you,\n",
            "For I hursend where Mustch our dose not\n",
            "take all notay:\n",
            "And, and men, mest thinks severy,\n",
            "This in infectiang much a would,\n",
            "And left braned his foInds, talking's dead of the anys\n",
            "In tombel mentrousions, thing breamn'd my have eate I discomf detio your How,\n",
            "Shamperelels ope. The oldreason,\n",
            "And deer of through he it; but but his forly.\n",
            "\n",
            "LARTENS:\n",
            "A Wenworly queen, musts beguar\n",
            "Good vonsure to you, must a bele hold;\n",
            "I the detnumty may's from you therein finsty eyes,\n",
            "But wouldly tonters smistom.\n",
            "\n",
            "HERMIORIOLANGALUS:\n",
            "Nas;\n",
            "River thou a-lates cornunt.\n",
            "\n",
            "PROARENCE:\n",
            "My flaring straribred wacle.\n",
            "\n",
            "CORIOLEY:Let you\n",
            "forth, Givile! Where manrward. Pray, as who, give affive you his,\n",
            "Amonaster I have hearth; commanry of their wife\n",
            "Onchion that heave out cold, offrom lives?\n",
            "Therefore twretere tonguio? But I hort burself?\n",
            "\n",
            "First:\n",
            "In the sputing our his requeak.\n",
            "What thou't nursed fornoish, many and sentry talbray.\n",
            "\n",
            "PORDINCE:\n",
            "Where parfority, with down'd fortunds, but whome his nightrave wit;\n",
            "And aboul that his lost to bet you not before amontand friest your aubid the enembding to Rurmon.\n",
            "Citins speed I'll told. If my I wenterd, man\n",
            "Belser; of\n",
            "Good, if this done,\n",
            "That heir beast minetham thy courten many\n",
            "holy fitend ne'ever tongure sucent that clamed,\n",
            "But then unterrioutysea, done, I adviey.\n",
            "\n",
            "VOLUMNIUS:\n",
            "Oapure boundly.\n",
            "Of next, is my becan mest da my pleach,\n",
            "And me too, and ress parth this,\n",
            "Well, I should movence aste-pirace:\n",
            "If it the yourr hornse sleep not teeds\n",
            "At but wasts formy helpels livives his not and thoughts!\n",
            "What care in that the\n",
            "speech and envy folly not.\n",
            "\n",
            "FindOR:\n",
            "I had 'dreat shall to the wretty.\n",
            "What crumpes.\n",
            "\n",
            "Merry I have slept:\n",
            "Why, that thy wingraning onection his furse tomb'd,\n",
            "By my son.\n",
            "Noy shroin kingdows;\n",
            "And if me that bittit still, Lord buty fates\n",
            "I wast to father allina\n",
            "Cry a knowly the prayeding win in Dontry.\n",
            "Op; I becar you. Yet you have with stome, that make in\n",
            "Purlated with you so?\n",
            "To know we by tombish, that jroy breath,\n",
            "But is a make, is throng the breath, fride,\n",
            "Againt thesese with quink whiles!\n",
            "Of smy been means, the whose\n",
            "You shall thy are wiltarduy agaire,\n",
            "How? steen not of drives him rightion'd\n",
            "Gham to me,\n",
            "Fells mour ormile, these love; for stray to you whom Three.\n",
            "\n",
            "FMARSS:\n",
            "Year welcomes your arparfact, says and\n",
            "Well was it.\n",
            "\n",
            "Then tYBRUKEN MOMARGARE:\n",
            "Near, two moranow fall, and and trohnst with Wa what'st,\n",
            "And have facess thou\n",
            "bow come our no dirterence, the fear of applage do go you againxpled.\n",
            "\n",
            "MARGARENCESTER:\n",
            "Wilthat our late, whom fralkingner:\n",
            "Or or thy brother, they, and veriang not that Lorden I saves.\n",
            "\n",
            "KING VINCENTIO:\n",
            "\n",
            "PARDIT:\n",
            "Alame not then thy lovings.\n",
            "\n",
            "MENENCENSTIO:\n",
            "May kneep todurpe: where hoursh soviling,\n",
            "She hadst yet Here'thon: majant,\n",
            "The gommen do sseems your ear the sack enter tends\n",
            "Warwe, God, I junt, who, what thou\n",
            "so aftergunce!\n",
            "Her loved, to sureed of you\n",
            "That thungst on thou thee?\n",
            "Furned My his love?\n",
            "If confer the worlENTEY:\n",
            "Onburn Richion much parking wroughters, and me as down, in will thy why.\n",
            "\n",
            "COMINIUS:\n",
            "Duke that you all Cannoly, forth:\n",
            "Tless, I like of thinkss 'fies\n",
            "His clattes. We well have hath thee thral poce to the sound is Alliecter,\n",
            "My was he yet, must armion.\n",
            "\n",
            "PUKESBY:\n",
            "I cit the discreht not younk, sile that brrowfrenterling is own.\n",
            " claw why we bother him yond, my bring bed to daince,\n",
            "To that man's doe, as is life griegn, there's blebase\n",
            "Think is it guity, may all the givom my wake\n",
            "should I so reld,\n",
            "Frongius nistit hate at. Iscan us.\n",
            "\n",
            "PENCUTEN NIARD IV:\n",
            "And, if that wast to chear is rathreian\n",
            "The westy wells; our, I way, Do this Volde best flife a inty\n",
            "In my meat? Prough to the sweet soisons the you,\n",
            "Dorthing, thou mastir.\n",
            "\n",
            "MARCIUS:\n",
            "That shall\n",
            "the room foners, Lut thy staddaw; is his no acont\n",
            "to made strielly your sled suil advine.\n",
            "\n",
            "MENENIUS:\n",
            "Madiery, you naught revile.\n",
            "Out him ply in with on thee you rumuses his forther?\n",
            "Do my gresaty merport\n",
            "And now she cittity such shall you sure being that\n",
            "Thus good his tire as fortune; both that\n",
            "'Tis make more advents threse they than sendry king?\n",
            "\n",
            "HERY BINGHNE?\n",
            "If comminetter womber, great's bearl-agon honest that\n",
            "-fathly, counsting, for that unto you arts,\n",
            "Their colditore sander ibus\n",
            "The kind meet, will to so brings.\n",
            "Pond, after men have to baninged quareber of this as his secested\n",
            "Is pay that us.\n",
            "\n",
            "PORINCE:\n",
            "O anster abours seir'd, my conterner:\n",
            "good to at face, as thy drost your heart drong as If if only of they.\n",
            "\n",
            "JOHNOHN OF YORK:\n",
            "Vouther'd not, now, Cadiole.\n",
            "That father! falturs, will mintellus, so your torge\n",
            "A he sunto\n",
            "Andoncarst I confort\n",
            "Montrung cousin, volity honed. Goodly's then cause on been\n",
            "Those cans, it? Juliet the doth,\n",
            "In youUKE OF YORK?\n",
            "\n",
            "ROMEO:\n",
            "No foold-Dukes me; and ray--a good weat your hards Ctive\n",
            "You many, thou? Roman.\n",
            "\n",
            "VORIOLAND:\n",
            "I it.\n",
            "Norfolk they, Here wast glint,\n",
            "Romans, was Keen of been you armingly then untersuom\n",
            "me.\n",
            "\n",
            "GLOUCESTER:\n",
            "God then any,---whath noble the gace.\n",
            "Nou Sashe thy love, but not is wear\n",
            "'Es forty:\n",
            "Out appet though pease?\n",
            "\n",
            "PULIET:\n",
            "Ressel the couldry breath this wishond race death,\n",
            "But detumner amstome, strept arm here,\n",
            "Or later of within show that love in gry on the hustated that him.\n",
            "\n",
            "CORIOLANUS:\n",
            "Yea scar:\n",
            "Throughsles: fou at; to twould but clonter\n",
            "she not in olde bungen a firsts.\n",
            "\n",
            "HORTIO:\n",
            "How If hercemeners:\n",
            "Aubertain.\n",
            "\n",
            "GRORISABELLA:\n",
            "RiquamS:\n",
            "It know-make of such, 'mout i' him you,\n",
            "uncusion from lifghts fut have mehburn,\n",
            "Whless sulab off; that morther, that thou his weell.\n",
            "\n",
            "LORD MARGARET:\n",
            "Do, musines alaps, my the game?\n",
            "That I apore make thou bay hand what's backnow.\n",
            "\n",
            "POLIXS:\n",
            "He putcomard; her! and frields, in my and ronly to friend that the sain,\n",
            "Montruend and shame fultbed, wrong I smily wafter ot be saway my courpany.\n",
            "\n",
            "VIRGBROKE VINCENTIO:\n",
            "What hom too fell in cry sprother of our't; gaints how,\n",
            "Joy there blood, day; you\n",
            "And I should.\n",
            "\n",
            "Pursomfagulies:\n",
            "'Ty marry consurest subdeeme\n",
            "Than conforcENTE:\n",
            "Niness, if heave sent uncronce makes is is kneeping I every acconsed on this oldies.\n",
            "\n",
            "JOHROKE:\n",
            "And in the nothin us! would,\n",
            "And to repare knother, forford\n",
            "As hereefuly his as of I man sist, as mine boy tallies.\n",
            "\n",
            "BENVOLIO:\n",
            "Ut we slame:\n",
            "Is aw! look Tyme wark come; put is hing;\n",
            "It worthing please-must bringlish'd\n",
            "But you say with with\n",
            "fortune of make fortunned her resont,\n",
            "Tell servingmans holt I not, brate\n",
            "Thou our toundnere anspity I cannot 'moutlaus,\n",
            "What is the enderabout but tay, fortate,\n",
            "And ope youraure withy crowning.\n",
            "\n",
            "GHORD VINGBROKE:\n",
            "Out have no twill young have thout upon to-morroad,\n",
            "What wenty cready's deemmn'd, and Wunto thou to'struck mance how Say,\n",
            "And lose they from first boot make mistred forgeth let then beasts.\n",
            "\n",
            "PUYCA:\n",
            "Of my siter to sut.\n",
            "\n",
            "GLOUCESTIO:\n",
            "Ticher, the scaughter to wronged:\n",
            "Away, my hontends buney, I truke, pardince the come with him?\n",
            "\n",
            "ANFORD:\n",
            "By livring addess at here, for forning of to this nurt:\n",
            "I have 't,---\n",
            "How to is a queen Mopartion a leet\n",
            "The bring to them's.--sweary with where us, fray, in good out,\n",
            "An have and toel; in with when make holds,\n",
            "And my live renong your now\n",
            "To their agarse too honstips;\n",
            "And anster smity hide atsits his took\n",
            "Here that been me.\n",
            "If mock my selfleen, corried will; not at comforts\n",
            "And While tomforving had behing, lay't\n",
            "King nevor\n",
            "Comian our throke, where heards purpose you but as his\n",
            "will as time us neess compory.\n",
            "There offirectal thou Isderven to\n",
            "This yould prouch looks your Londs?\n",
            "\n",
            "LEORIZEL:\n",
            "Come?\n",
            "\n",
            "CORIOLANA:\n",
            "Cry blood I, my I am sour down they buning of he scerven before one your mahn.\n",
            "\n",
            "Givartan, I is but cannot take his wis is leftly all,\n",
            "Mister; letself? made upour honsour Return:\n",
            "Stay dost some wents, Gream: and our thy stolved\n",
            "Most I know our gord them tame!\n",
            "His wooth, or man, my belinds.\n",
            "Are is some your hond,\n",
            "Or Vempins fathortwixt leens-man,\n",
            "Will in buy desping of heang.\n",
            "\n",
            "QUEN OF?\n",
            "\n",
            "AUMERLAND:\n",
            "Yet us louke.\n",
            "\n",
            "Nover I Love will ingminining.\n",
            "\n",
            "COMINIUS:\n",
            "Nay, to again;\n",
            "And that took at hot sleems am.\n",
            "Foolst; my bettens.\n",
            "Look! it canius of his wombs.\n",
            "Sitle hops'd that withile the dead, I willses it. To, suchomes thy steepmites some; come.\n",
            "\n",
            "KING RIVER:\n",
            "His what trengt, First the very wantimening sack'd would, there mistroom Que and\n",
            "They out:\n",
            "That not now whom, murdon, you?\n",
            "\n",
            "JOHN GAREYt:\n",
            "Which, laitt, now is him.\n",
            "\n",
            "GHENMARY:\n",
            "The sein that before loved thou two are drived shopate,\n",
            "Stlemmited us of you think ensurn him?\n",
            "That not the wittray, not, but was king in the man old.\n",
            "\n",
            "NORTHUMBERLANTENLBUA:\n",
            "A Jawhary to detutines.\n",
            "Would kity restn: I am! I waster!\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "O entoster, all and good Mespecorn.\n",
            "\n",
            "TROMEO:\n",
            "Only then his whilther,\n",
            "Grehaning toughar a drepays opet thus?\n",
            "\n",
            "LORUS:\n",
            "They my is hinds,\n",
            "Ouburn a blined that? where Of a hishavign's somed-fortune\n",
            "That may touffenced I sluing to pack\n",
            "Now you Rome'st my son is\n",
            "Toll been my then, Margity?\n",
            "\n",
            "First God lark not to lough I -spon of God\n",
            "Yies. him, to begar!\n",
            "Of the our the llord, whose entuness\n",
            "Andion, my I have a mownientands you art\n",
            "Offitdely the kingpring of the'st that give feehts, my our debitely that live,\n",
            "Gettern's glooks orse founteth eright.\n",
            "\n",
            "GERCUMBELLA:\n",
            "Nour'd:\n",
            "You,\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "That texes admsed won or him. Warwick the man.\n",
            "\n",
            "VORIO:\n",
            "Come:\n",
            "Prountsed but before burge I, doungradian't now,\n",
            "Thrich villast: why striples yourard have as beain,\n",
            "By! mistry's death diest it brittly,\n",
            "Of we sabless not be and toncursin,\n",
            "The fatiouty. nike hund! What seess.\n",
            "\n",
            "FROMEO:\n",
            "wentaten, death Stream Oxprounder Some broth'd:\n",
            "Hath you honter agres and so one the fwilt'd mont.\n",
            "\n",
            "Nurse:\n",
            "Give rency,\n",
            "Shat sures confoetion my my fair thus,\n",
            "To-fathpirined I beteens; my hengely as nispown,\n",
            "There plasone: If\n",
            "What, thou down that it marroal recentret,\n",
            "Stand old you is death; do love,\n",
            "Ay, I we supporth as nighth the houb\n",
            "and trother, if beat; sweeph that exquaren! Iffell come trumph.\n",
            "\n",
            "VOLOHY:\n",
            "His honour, worthere's for with is his nows be and I.\n",
            "3 Proters. my gentlem'd seek: is my on lotle:\n",
            "Mideweed to that some tough that though outh? I what'd as hrongured upon fortune, heartle.\n",
            "\n",
            "CORIOLANUNE:\n",
            "Whilthy her what\n",
            "Fartues fewill, sray welling breforn,\n",
            "Live in is mark't awhile in as to arcarge sith be itles, my unstroy thlurt'ss the encomphrerated, hou?\n",
            "\n",
            "FROMIUS:\n",
            "Traliy, thou have in his stold my eing of that's neck,\n",
            "That my have not were my some thrither.\n",
            "\n",
            "FROMIUS:\n",
            "No braid:\n",
            "Cry tongue touble unly then myself.\n",
            "\n",
            "CLARENCE:\n",
            "A teady, dry, sirreford's offenceed,\n",
            "Nor fachy in myself by some vappass'd?\n",
            "\n",
            "ANGOLOUS:\n",
            "Is unme drevened upon poy sit and his seen.\n",
            "\n",
            "MENEN'T:\n",
            "Straithor: stay, when, drangele,\n",
            "Andly shamed for God a it, I good that loving.\n",
            "There, him styleing the not smarer a mine\n",
            "Told's best, therefore of forture to\n",
            "For capess, as not a creedint,\n",
            "And becospart this togentomes. Vinity discannot.\n",
            "\n",
            "QUEEN MARGALET:\n",
            "Are I hold. Take is Glord, but proy\n",
            "Comfio guestleings muchstriing apsire:\n",
            "Good, no tounse toldryant.\n",
            "Are and 'twere I wom.\n",
            "\n",
            "Serposent Of Purse:\n",
            "And, I as I Bauntany rever: love,\n",
            "On was ell behapose the pray'd,\n",
            "Award that I his that world may our soldies\n",
            "Before the their well want; as brother miltle,\n",
            "'For Bodwells I wash tears you last;\n",
            "Iest it adve atters thou fortune.\n",
            "\n",
            "FROMAR TARBY:\n",
            "And must be me but in as the asnurne of returions\n",
            "And not now that the woulself. dest you frow sound,\n",
            "Shar broke and to'st the his uprain's mustry\n",
            "But fair I drourth's wife,--whose my have an tminer! cervingry,\n",
            "I world tommin his standed, ne'el stinderling burnonent no oness to the fastrowns allign his thou pray,\n",
            "To bear his dreal mele, and frierns,\n",
            "on mother answy,.\n",
            "\n",
            "VING RICHARDORCENTIO:\n",
            "Stay I that your work buh his dood\n",
            "For it, your granced untursh:\n",
            "Nuch this loybale servingman:\n",
            "you sie? I'll crown. A Phetion\n",
            "the done of in our have the man.\n",
            "\n",
            "OXFARD AWARFORS:\n",
            "Have's on bring thus, thou welpose sortagumitious:\n",
            "Ol like be; you\n",
            "Then me now fallity, but\n",
            "that that the wiolt ungrage to fortun,\n",
            "Colland with dervild and if accuse graven all paulife,\n",
            "And heavenUol, Camind to\n",
            "to his, I have majests tarry hath\n",
            "down you uselfiering light: I thost whose than a marring;\n",
            "Butle penumand fLan thee talt in rapnider,\n",
            "Where's pilecinstilate vollantandious,\n",
            "To wingin these now.\n",
            "\n",
            "La!\n",
            "IISABELLA:\n",
            "Our you I what wascann'd:--\n",
            "pears, no, and is speen unto from unsemn.\n",
            "\n",
            "POLIXGect.\n",
            "Which then andselfity, what all me:\n",
            "On you arm not some: but anter who faithar unwortay,\n",
            "Andrell your some fromb doath, here,\n",
            "Your stome to, a will up is deniecty mersed and assilant of.\n",
            "\n",
            "DUKE VINNENS:\n",
            "How none it. Is bradge they disclended:\n",
            "For it and what unmpainter at toes.\n",
            "\n",
            "GLOUCESTER:\n",
            "What him his sgoes jaction,\n",
            "Ket so'd muchmank, but as at not myself\n",
            "As to bohfory to noly;\n",
            "Our holess it youy give innog to or doth-pleace?\n",
            "\n",
            "GLOUCESTER:\n",
            "A Ifford?\n",
            "\n",
            "HENRY B:\n",
            "Dhis forthings:\n",
            "Failly sby youth faithy thrie beholky the sentraed to neide call what Ase,\n",
            "Paletter on enchedfule, heart yet you How?\n",
            "\n",
            "MANGARES:\n",
            "Yet see:\n",
            "Is I will approorieds.\n",
            "\n",
            "KING VING HENRY BOLINGBROKE:\n",
            "Nor ship come wilthre the goven myselfil grain's come\n",
            "And if deep'tn, but my nother, forthen these see fair mine. I demird;\n",
            "Cattele dreams drumeries God But out smervantion.\n",
            "Sir,--\n",
            "\n",
            "BUCKINGHAS II:\n",
            "Leive him Yone hand\n",
            "temphing smilarianced ost with arm'dly\n",
            "Would may, nume sinnees he were is.\n",
            "Nount napity dreats word armsoment.\n",
            "\n",
            "QORUMGBRUKE:\n",
            "In douth, simes, repent man thy haste you art quesn. His plooring mighty, where to connot.\n",
            "Thy leats as is the this partreph.\n",
            "\n",
            "RUTY:\n",
            "No hearth it?\n",
            "\n",
            "QUEEN FIUS:\n",
            "Will some bewittle, if your\n",
            "Of the sayly; where I cames as friendrys daught confy them of out\n",
            "stea?\n",
            "Of nof beneed,\n",
            "Intelle been, I have, I out sling Mamnerthly than to was in willantence to a his whe would fortune hurd.\n",
            "\n",
            "Praok VI:\n",
            "Well unather, I I have tranger! failes\n",
            "To his nother living his mone kingpleasine:\n",
            "Fishal they the peause may my sit.\n",
            "\n",
            "KING CARD II:\n",
            "My must I blay, we his bounds seems.\n",
            "\n",
            "MORIARENCULER:\n",
            "Ot, and a vittons, they not he trut out amb,\n",
            "Which belt keep you; not be it stady?\n",
            "\n",
            "BUCKINGHAM:\n",
            "By ear his not now how monte countruckfult,\n",
            "Thou I world againt, for have mile is myself\n",
            "Wids; but I dasire, cancle arms,\n",
            "And affitly my vases death.\n",
            "\n",
            "VIONUS:\n",
            "Dow shout havenst With one\n",
            "nead all to his that I man out,\n",
            "I--o manne it the Siringman:\n",
            "Ghost us say hath I tward you 'twaring myself, to loove thy arth Of thou as whe sitte\n",
            "Go up a pray measy ween I. Which to sea.'\n",
            "Why usmonc mastess of you resour:\n",
            "If husbows thee, let I have sut the seach\n",
            "Toldere not\n",
            "clavinstroat unather-rardior, sI's in well manusine,\n",
            "Untwere mistruct sils? nob be\n",
            "Did garshamoon:\n",
            "It tooste Busa's stringing\n",
            "The manvortual in wasty longs make\n",
            "Is frewere coply tobes this preasedsion'd\n",
            "This misterfect; I were too any?\n",
            "\n",
            "BUCKING HENRY BOLINGBROKE:\n",
            "Hark'\n",
            "They, that forth, would suchland done. may.\n",
            "\n",
            "BORD GARENCENt OF ANA:\n",
            "Woulds I any ever,\n",
            "Turse men it.\n",
            "\n",
            "QUEEN, when BARHNARD:\n",
            "What's buttell, I what goes your bride?\n",
            "\n",
            "LORD NORLAMNENLE:\n",
            "O that, dids but his bear;\n",
            "Stared thus;\n",
            "So sowrather then I dost.\n",
            "\n",
            "MENENIUS:\n",
            "And'stardens beads his body.\n",
            "\n",
            "Slause to\n",
            "The loess, hench mancan you wile your watesdom my brown'd\n",
            "Stold mistranted and thou,\n",
            "Bessivint Firsham threin becom as you fathy'sway.\n",
            "Wall. Prout be, no yet\n",
            "speik, most thy purce.\n",
            "\n",
            "POY SABELLA:\n",
            "Go seemery infatuny:\n",
            "Flycees, thy were husbands: I betweerf? the flyst thus he.\n",
            "\n",
            "BALLA:\n",
            "It is him to speeks.\n",
            "\n",
            "KING HENRY VI:\n",
            "Ithouts, and yet strong\n",
            "anded tell upor and poide my nade:\n",
            "Hus dove your toke, thy much\n",
            "Preforge, and I have on, his your that dry;\n",
            "As you have hond'st the vaolies.\n",
            "\n",
            "QUEEN MARCETLLA:\n",
            "My leaves\n",
            "They en the having haste it.\n",
            "Away drongs to betweers here where is but your storn\n",
            "Thlese heavought spy burt asches, thou state.\n",
            "\n",
            "PRUTUS:\n",
            "Is thou old Cityn's him; therefore;\n",
            "Remicatio, I will thee? Hamm; the Camite.\n",
            "\n",
            "First Affort Ruchess:\n",
            "I'll bewere high, that heve as\n",
            "Illow.-\n",
            "Poswear no me. Young Jace, no toughn been\n",
            "me of demp:\n",
            "Made, Groyal that the grant your,\n",
            "I hone sinford, will I bexetpinens, unfrom becaugh'd 'tis bay,\n",
            "What make thiches fherefore of the part:\n",
            "who\n",
            "I'll now unneyour,\n",
            "That let, win becond with they imperion\n",
            "Hell old upoz that's begarter in live.\n",
            "My selfeephery, bring and growd!\n",
            "It fries younds batte's gribey?\n",
            "\n",
            "PRUCHESS OF NORNCE:\n",
            "Thou your virtal to's hear all.\n",
            "\n",
            "ISABELLA:\n",
            "God this may, Brest we to love, poless exprow;\n",
            "If that atters a fraign him anded-drow.\n",
            "\n",
            "LARTESBY:\n",
            "Then! will thou bettear\n",
            "But! whose smerve'd the be Ast.\n",
            "I'take glive mester Isall me;\n",
            "It I would forth, meetan her, I\n",
            "That on mouther Morner:\n",
            "Not is not misfacian 'twountin-by not,\n",
            "MyNarth, but out face, where yet rance took cry the sraple.\n",
            "\n",
            "LDUCENTIO:\n",
            "Petian you not deedings nattla,\n",
            "I shall not it begir bout accaze.\n",
            "\n",
            "JULIEY:\n",
            "A prome in Gentleman,\n",
            "As withen this provatiol. Which Towerd feath doth\n",
            "father a truchion that but that with parCaze\n",
            "thee, boy and the not of dratten you:\n",
            "It forge will you to mich the, gone;\n",
            "Will were and some to to the seun told Varple\n",
            "With he mean armicned of must sort I.\n",
            "\n",
            "Thing Is that mult he astvor,\n",
            "Not inglazed in undeeps being a gurner,\n",
            "With here, his; Kour'll--tainnurt:\n",
            "I that that to smalllow'd their I feesee not know fly litsty,\n",
            "That holt of yourses togs it;\n",
            "And thon all brother live O; onether.\n",
            "\n",
            "RICHORD NORYCUS:\n",
            "Farking.\n",
            "Oursh, and bettal\n",
            "wither untomgurne servie'd osse behanims.\n",
            "\n",
            "FRIAR, the rroson'd warst that\n",
            "Prove Bohfail'tis conbented, Marget:\n",
            "Go whom not you fair?\n",
            "\n",
            "NORFORY:\n",
            "The beary I knorger, Was you my not\n",
            "to the too, for so and beas tilever'd they but will pawer tost I--\n",
            "Far youer drumpiter, thou lovet but bist;\n",
            "And,\n",
            "My Daish thace ifde thy butch'd to these said\n",
            "Of beggetwer munsckled make, there hew is you,\n",
            "Havi day dethose of the cland-dray.\n",
            "Why, my loness oor not's he spirit,\n",
            "Say, set aunster saturne,\n",
            "Hathy kind fivir; I'll foot this smate,\n",
            "And thy leave a vile Cwere womTout grace!\n",
            "Herly seem fortune to me moscomp'd,\n",
            "Or valittess they that nwand oname\n",
            "Of a thou nisham thee,\n",
            "Haveing sentempanio myselfing fixt.\n",
            "\n",
            "HERMIONARE:\n",
            "Cay!\n",
            "\n",
            "DUKE OF CAMELLIUS:\n",
            "Thought in these of Glit, gum reave!\n",
            "\n",
            "MARCUSTAND:\n",
            "You dish there soul?\n",
            "Now 'swears too took upon,\n",
            "As I, butght goole have on.\n",
            "A their and thou heaven yursuon MRonsince and Citizen I becer Guse his clite twas the worngh whither\n",
            "And resss in curses; noting will, have I my minteen, my crown,\n",
            "I agaile sweet for more housten of is his\n",
            "Which heavy'toncent of there the bold\n",
            "But, this block'sabed tongue; but lies weep?\n",
            "To woman us fatish terry.\n",
            "Where's he the shall rouds.\n",
            "On; I my see such if thence Sdone: bot my tomouthBding'd\n",
            "Bot. Yet lent theirs; who nattle. I am and his regom arm Less thou upon the fatorn,\n",
            "Or and honiar comperenom to-man\n",
            "Thou crownest what is my voost where I will:\n",
            "Or drurther's forther, dirturtutces\n",
            "Our night done their poissuren bewe in to know\n",
            "haviening our peace, me\n",
            "Ladies to they curpits\n",
            "I his vaulances my tontrantor:\n",
            "Then, my let at in that his me noft,\n",
            "for him, tithen, none I am know all now,\n",
            "Makent what it that that reasts! anmottrums to those?\n",
            "\n",
            "HORTHUCENSAR:\n",
            "Baint bring the clame. I set, if your which away.\n",
            "\n",
            "BetwIUMERLE:\n",
            "I far it, comforties. IV that the whithen us our,\n",
            "Are shame thy turnhinkstriouss'd you;\n",
            "As hone to must untalgetourning,\n",
            "Norful as away to that state.\n",
            "\n",
            "ROMEO:\n",
            "That day are of Baudely your you\n",
            "Lund in you ba\n"
          ]
        }
      ],
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "e1ebc6ce21e20a44ca2e939d468f8c4914f49f277b8be916293753b9e7a9feef"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
